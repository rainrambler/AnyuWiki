- 联邦学习是一种机器学习方法，它使机器学习模型能够从位于不同站点（例如本地数据中心、中央服务器）的不同数据集中获取经验，而无需共享训练数据。 这允许个人数据保留在本地站点中，从而降低了个人数据泄露的可能性。
- **什么是联邦学习？**
- 联邦学习用于通过使用多个本地数据集而不交换数据来训练其他机器学习算法。这允许公司创建共享的全球模型，而无需将训练数据放在中心位置。
- 联邦学习是机器学习领域的一个新研究课题。在 2015 年特别是电信领域的研究之后，人们对联邦学习的兴趣有所增加。2017 年的一篇 Google AI 帖子进一步增加了兴趣。联邦学习最有可能成为一个活跃的研究课题。联邦学习的研究可以根据机器学习领域对先进的新学习过程/架构的需求进行扩展。
- **联邦学习有什么好处？**
- 联邦学习是机器学习领域的一个新兴领域，与传统的集中式机器学习方法相比，它已经提供了显着的优势。 联邦学习的好处是：
	- 数据安全性：将训练数据集保存在设备上，因此模型不需要数据池。
	- 数据多样性：数据安全以外的挑战，例如边缘设备中的网络不可用，可能会阻止公司合并来自不同来源的数据集。 即使在数据源只能在特定时间进行通信的情况下，联邦学习也有助于访问异构数据。
	- 实时持续学习：使用客户数据不断改进模型，无需聚合数据进行持续学习。
	- 硬件效率：这种方法使用不太复杂的硬件，因为联邦学习模型不需要一个复杂的中央服务器来分析数据。
- **联邦学习的挑战是什么？**
	- 投资需求：联邦学习模型可能需要节点之间频繁的通信。这意味着存储容量和高带宽是系统要求之一。
	- 数据隐私：
		- 在联邦学习中，数据不是在单个实体/服务器上收集的，有多个设备用于收集和分析数据。这可以增加攻击面；
		- 即使只有模型而不是原始数据与中央服务器通信，模型也可能被逆向工程以识别客户端数据。差分隐私、安全多方计算和同态加密等隐私增强技术可用于提高联邦学习的数据隐私能力。
	- 性能限制：
		- 数据异构性：来自不同设备的模型被合并以在联邦学习中构建更好的模型。设备特定特性可能会限制某些设备的模型的泛化，并可能降低模型下一版本的准确性。
		- 间接信息泄露：研究人员考虑了联邦成员之一可以通过在联合全局模型中插入隐藏后门来恶意攻击其他成员的情况。
		- 联邦学习是一种相对较新的机器学习过程。需要新的研究和研究来提高其性能。
	- 集中化：联邦学习仍然存在一定程度的集中化，其中一个中央模型使用其他设备的输出来构建一个新模型。研究人员建议使用区块链联邦学习 (BlockFL) 和其他方法来构建联邦学习的零信任模型。
- **联合学习的替代方案是什么？**
- 八卦学习（Gossip Learning）被提出来解决训练数据隐私的相同问题。这种方法是完全分散的，没有用于合并来自不同位置的输出的服务器。本地节点直接交换和聚合模型。与联邦学习相比，八卦学习的优势在于其对基础设施和集中化的要求更少。八卦学习是一种新颖的方法，需要进一步研究以提高其性能和稳定性。
- https://research.aimultiple.com/federated-learning/